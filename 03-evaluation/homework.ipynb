{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d92324-ae51-4913-8bc7-64e2560ee9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minsearch in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.0.2)\n",
      "Collecting minsearch\n",
      "  Downloading minsearch-0.0.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: qdrant_client in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.14.3)\n",
      "Collecting qdrant_client\n",
      "  Downloading qdrant_client-1.15.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from minsearch) (2.2.4)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from minsearch) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from minsearch) (1.6.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from qdrant_client) (1.73.1)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /home/codespace/.local/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant_client) (0.28.1)\n",
      "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from qdrant_client) (2.10.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from qdrant_client) (6.31.1)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from qdrant_client) (2.11.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /home/codespace/.local/lib/python3.12/site-packages (from qdrant_client) (2.3.0)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant_client) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->minsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->minsearch) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->minsearch) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->minsearch) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->minsearch) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->minsearch) (3.6.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (4.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->minsearch) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.3.1)\n",
      "Downloading minsearch-0.0.4-py3-none-any.whl (11 kB)\n",
      "Downloading qdrant_client-1.15.0-py3-none-any.whl (337 kB)\n",
      "Installing collected packages: minsearch, qdrant_client\n",
      "  Attempting uninstall: minsearch\n",
      "    Found existing installation: minsearch 0.0.2\n",
      "    Uninstalling minsearch-0.0.2:\n",
      "      Successfully uninstalled minsearch-0.0.2\n",
      "  Attempting uninstall: qdrant_client\n",
      "    Found existing installation: qdrant-client 1.14.3\n",
      "    Uninstalling qdrant-client-1.14.3:\n",
      "      Successfully uninstalled qdrant-client-1.14.3\n",
      "Successfully installed minsearch-0.0.4 qdrant_client-1.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U minsearch qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9bba44-c5fa-4310-a1c2-7400c2cfb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
    "documents = requests.get(docs_url).json()\n",
    "\n",
    "ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ded83-a1d4-4448-ad29-802b543edd3b",
   "metadata": {},
   "source": [
    "### retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d96b18b-16c7-4a23-8887-3ded70a15003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "    \n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "    # for q in ground_truth:\n",
    "        # print(q)\n",
    "    #     break\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "        # 'relevance_total': relevance_total\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "345b6acc-3a8e-4525-b8ff-6378b7374c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7dd03368a390>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set minsearch\n",
    "import minsearch\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\", \"id\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb0eca28-8e01-4366-81f6-bf88b28ccb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search(q, boost = {'question': 1.5, 'section': 0.1}):\n",
    "\n",
    "    results = index.search(\n",
    "        query=q['question'],\n",
    "        filter_dict={'course': q['course']},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7cca63-426f-4b68-b222-539e6a4b600a",
   "metadata": {},
   "source": [
    "## Q1. Minsearch text\n",
    "\n",
    "Now let's evaluate our usual minsearch approach, but tweak the parameters. Let's use the following boosting params:\n",
    "\n",
    "```boost = {'question': 1.5, 'section': 0.1}```\n",
    "\n",
    "What's the hitrate for this approach?\n",
    "\n",
    "- 0.64\n",
    "- 0.74\n",
    "- 0.84\n",
    "- 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fea700f1-75e2-463c-9014-273104ae0375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4627/4627 [00:14<00:00, 315.35it/s]\n"
     ]
    }
   ],
   "source": [
    "boost = {'question': 1.5, 'section': 0.1}\n",
    "\n",
    "minisearch_evaluation_results = evaluate(ground_truth, minsearch_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3fd6e967-f6e5-4d24-9974-88c7bd446d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.848714069591528, 'mrr': 0.7288235717887772}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minisearch_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5b89e-3150-49fa-97a9-8461fb7bcbec",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "20d237ce-70cd-4505-8aba-fe33edf8006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import VectorSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65710cce-89fe-43ce-9967-e9cdf610ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "884398b4-59c6-4565-a621-94d1b0036e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings for the \"question\" field:\n",
    "\n",
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question']\n",
    "    texts.append(t)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6b810ffd-4e42-496f-99c2-0bf710a2f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings for the \"question\" in ground_truth dataset:\n",
    "\n",
    "ground_truth_questions = [None] * len(ground_truth)\n",
    "\n",
    "for i, q in enumerate(ground_truth):\n",
    "     ground_truth_questions[i] = q['question']\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "Y = pipeline.fit_transform(ground_truth_questions)\n",
    "\n",
    "for i, q in enumerate(ground_truth):\n",
    "    q['vector_question'] = Y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e6f73-e866-4e8b-8a4f-560368145290",
   "metadata": {},
   "source": [
    "## Q2. Vector search for question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f820e-8830-40b1-adf5-7c6bac70ecd0",
   "metadata": {},
   "source": [
    "Now let's index these embeddings with minsearch:\n",
    "\n",
    "```\n",
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)\n",
    "```\n",
    "\n",
    "Evaluate this seach method. What's MRR for it?\n",
    "\n",
    "- 0.25\n",
    "- 0.35\n",
    "- 0.45\n",
    "- 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "346782e8-afab-48f9-bbbe-5042074b1c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x7dd030a0d3a0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b20094de-f18d-4776-b6cc-ecd1d5e32641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search_vector(q):\n",
    "   \n",
    "    results = vindex.search(\n",
    "        query_vector=q['vector_question'],\n",
    "        filter_dict={'course': q['course']},\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d61a6e86-20d4-4b68-ae40-92eb367ae4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4627/4627 [00:03<00:00, 1261.62it/s]\n"
     ]
    }
   ],
   "source": [
    "minisearch_vector_evaluation_results = evaluate(ground_truth, minsearch_search_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f0608170-8abf-4561-aa3e-11f9e5004c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.014047979252215258, 'mrr': 0.006829479144153877}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minisearch_vector_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e802c0-aa8c-4edb-a150-bf257821f1f1",
   "metadata": {},
   "source": [
    "## Q3. Vector search for question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "34b633d1-c37c-4b1f-8926-58ff87955814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When does the course begin?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'document': 'c02e79ef',\n",
       " 'vector_question': array([ 1.48205339e-01,  5.94105488e-02, -5.95477382e-02, -1.32751999e-01,\n",
       "        -4.15999152e-03,  2.02596703e-02,  2.14582013e-02, -1.02590879e-02,\n",
       "         6.20179196e-02,  6.69182872e-02, -4.27196998e-02, -1.99597864e-01,\n",
       "         1.58438991e-01, -1.23168138e-01,  1.68281992e-01, -3.57977343e-02,\n",
       "         1.67460232e-01,  8.76489584e-02, -1.26572384e-02, -7.19601119e-02,\n",
       "         2.49481472e-02, -1.62304123e-02, -3.19982042e-02,  1.49469925e-01,\n",
       "         6.72919505e-02, -6.59383833e-02, -1.21652259e-01,  2.01153946e-02,\n",
       "        -2.87345040e-02,  2.44779396e-02, -8.03484834e-04, -6.68915774e-03,\n",
       "         4.24317489e-03,  5.83691687e-02,  1.55151520e-01,  7.60471098e-02,\n",
       "         1.05027501e-01, -1.07203345e-01,  1.48006404e-02,  4.46888752e-02,\n",
       "         5.26749491e-02,  3.25053031e-02,  3.50181671e-02, -4.69688168e-02,\n",
       "        -5.68644234e-02,  1.61933951e-02, -1.30343706e-02,  3.81210272e-02,\n",
       "        -9.40065540e-02, -7.32850362e-03,  2.24985137e-02,  5.29043275e-02,\n",
       "         6.09013120e-03,  7.22785631e-03,  3.76614526e-02, -3.09176848e-02,\n",
       "        -2.95258709e-02,  3.28459044e-02,  2.52372639e-02, -1.13210681e-02,\n",
       "        -9.03733480e-03,  5.07409083e-02,  4.96033859e-03, -4.62757977e-02,\n",
       "         3.36145699e-03, -5.15283184e-02,  2.21571383e-02, -1.30796489e-02,\n",
       "        -3.61815472e-02,  1.55758910e-02,  1.74118483e-02, -9.07034132e-03,\n",
       "         1.59372259e-02,  3.41747630e-06, -2.94227464e-03,  1.72495621e-03,\n",
       "         1.64068488e-02, -3.01137559e-02,  3.90887993e-03, -1.77473126e-03,\n",
       "         4.04902988e-02, -3.60816100e-02, -1.32711758e-02, -8.49082250e-03,\n",
       "         2.06363484e-02, -5.37115164e-04,  6.46617265e-03, -1.07064803e-02,\n",
       "         1.26483645e-02,  1.95771017e-02, -7.12428526e-03,  8.40928596e-03,\n",
       "         8.51410459e-03,  1.23381081e-02,  3.54802533e-02, -3.42227195e-02,\n",
       "        -1.01742245e-03,  4.00264049e-02, -1.29677731e-02, -2.48669417e-03,\n",
       "         6.45440867e-03,  1.77110753e-03,  4.47632096e-03,  5.85619917e-03,\n",
       "        -2.00073459e-02, -1.27773370e-02,  2.62551969e-02, -7.89970929e-03,\n",
       "         1.45354622e-02,  2.68909073e-02, -5.80548578e-03, -9.83998776e-03,\n",
       "        -5.64243467e-03,  1.11400418e-02,  2.15234268e-02,  1.06782372e-02,\n",
       "        -1.38536925e-02, -2.25533175e-03, -2.68776176e-02, -9.87521009e-04,\n",
       "         7.89996915e-03,  3.87770521e-03, -2.06842551e-02, -2.10424701e-02,\n",
       "         2.44501974e-02,  2.14671102e-02,  1.81467086e-02, -9.90849237e-03])}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ccc16562-09d2-4398-b260-03c24fc10603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Install SDKMAN:\\ncurl -s \"https://get.sdkman.io\" | bash\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\\nsdk install java 11.0.22-tem\\nsdk install spark 3.3.2\\nOpen a new terminal or run the following in the same shell:\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nVerify the locations and versions of Java and Spark that were installed:\\necho $JAVA_HOME\\njava -version\\necho $SPARK_HOME\\nspark-submit --version',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1ac2c13c'},\n",
       " {'text': '✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\\nTo create the SparkSession:\\nspark = SparkSession.builder.master(\\'local[*]\\') \\\\\\n.appName(\\'spark-read-from-bigquery\\') \\\\\\n.config(\\'BigQueryProjectId\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\\'BigQueryDatasetLocation\\',\\'de_final_data\\') \\\\\\n.config(\\'parentProject\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\\\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\\\\n.config(\"spark.driver.memory\", \"4g\") \\\\\\n.config(\"spark.executor.memory\", \"2g\") \\\\\\n.config(\"spark.memory.offHeap.enabled\",True) \\\\\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\\\\n.config(\\'google.cloud.auth.service.account.json.keyfile\\', \"google_credentials.json\") \\\\\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\\\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\\\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\\\\n.getOrCreate()',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'cabe8a5b'},\n",
       " {'text': 'The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\\nexport PATH=\"$JAVA_HOME:$PATH\"\\nConfirm that your path was correctly set by running the command: which java\\nYou should expect to see the output:\\n/opt/homebrew/opt/openjdk/bin/java\\nReference: https://docs.brew.sh/Installation',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Setting JAVA_HOME with Homebrew on Apple Silicon',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c5de1f96'},\n",
       " {'text': 'If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps\\nInstall OpenJDK 11,\\non MacOS: $ brew install java11\\nAdd export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\\nto ~/.bashrc or ~/zshrc\\nActivate working environment (by pipenv / poetry / conda)\\nRun $ pip install pyspark\\nWork with exercises as normal\\nAll default commands of spark will be also available at shell session under activated enviroment.\\nHope this can help!\\nP.s. you won’t need findspark to firstly initialize.\\nPy4J - Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\\nIf you\\'re getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You\\'re most likely using incompatible versions of the JDK or Python with Spark.\\nAs of the current latest Spark version (3.5.0), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan! on macOS or Linux environments\\n\\n$ sdk install java 17.0.10-librca\\n$ sdk install spark 3.5.0\\n$ sdk install hadoop 3.3.5\\nAs PySpark 3.5.0 supports Python 3.8+ make sure you\\'re setting up your virtualenv with either 3.8 / 3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 for now as not all libs in the data-science/engineering ecosystem are fully package for that)\\n\\n\\n$ conda create -n ENV_NAME python=3.11\\n$ conda activate ENV_NAME\\n$ pip install pyspark==3.5.0\\nThis setup makes installing `findspark` and the likes of it unnecessary. Happy coding.\\nPy4J - Py4JJavaError: An error occurred while calling o54.parquet. Or any kind of Py4JJavaError that show up after run df.write.parquet(\\'zones\\')(On window)\\nThis assume you already correctly set up the PATH in the nano ~/.bashrc\\nHere my\\nexport JAVA_HOME=\"/c/tools/jdk-11.0.21\"\\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\\nexport HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\\nexport PATH=\"${HADOOP_HOME}/bin:${PATH}\"\\nexport SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\\nYou also need to add environment variables correctly which paths to java jdk, spark and hadoop through\\nGo to Stephenlaye2/winutils3.3.0: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com), download the right winutils for hadoop-3.2.0. Then create a new folder,bin and put every thing in side to make a /c/tools/hadoop-3.2.0/bin(You might not need to do this, but after testing it without the /bin I could not make it to work)\\nThen follow the solution in this video: How To Resolve Issue with Writing DataFrame to Local File | winutils | msvcp100.dll (youtube.com)\\nRemember to restart IDE and computer, After the error An error occurred while calling o54.parquet.  is fixed but new errors like o31.parquet. Or o35.parquet. appear.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Java+Spark - Easy setup with miniconda env (worked on MacOS)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e86ca928'},\n",
       " {'text': 'Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\\nSolution:\\n(Source)\\nVS Code\\n→ Explorer (first icon on the left navigation bar)\\n→ JAVA PROJECTS (bottom collapsable)\\n→  icon next in the rightmost position to JAVA PROJECTS\\n→  clean Workspace\\n→ Confirm by clicking Reload and Delete\\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\\nE.g.:\\nYou can also add classes and packages in this window instead of creating files in the project directory',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: Tests are not picked up in VSCode',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'bea22953'}]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = minsearch_search_vector(ground_truth[0])\n",
    "test_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
